[
  {
    "name": "Path-to-production mapping",
    "ring": "Adopt",
    "quadrant": "Platforms",
    "isNew": "TRUE",
    "description": "<p>Although <strong>path-to-production mapping</strong> has been a near-universal practice at Thoughtworks since codifying <em><a href=\"https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912\">Continuous Delivery</a></em>, we often come across organizations unfamiliar with the practice. The activity is most often done in a workshop with a cross-functional group of people —  that includes everyone involved in designing, developing, releasing and operating the software — around a shared whiteboard (or virtual equivalent). First, the steps in the process are listed in order, from the developer workstation all the way to production. Then, a facilitated session is used to capture further information and pain points. The most common technique we see is based on <a href=\"https://en.wikipedia.org/wiki/Value-stream_mapping\">value-stream mapping</a>, although plenty of <a href=\"https://caroli.org/en/path-to-production/\">process map</a> variants are equally valuable. The activity is often eye-opening for many of the participants, as they identify delays, risks and inconsistencies and continue to use the visual representation for the continuous improvement of the build and deploy process. We consider this technique so foundational that we were surprised to discover we hadn't blipped it before.</p>"
  },
  {
    "name": "Component visual regression testing",
    "ring": "Trial",
    "quadrant": "Platforms",
    "isNew": "FALSE",
    "description": "<p><a href=\"/radar/tools/visual-regression-testing-tools\">Visual regression testing</a> is a useful and powerful tool to have in your toolbox, but it has a significant cost given it's done for the entire page. With the rise of component-based frameworks such as <a href=\"/radar/languages-and-frameworks/react-js\">React</a> and <a href=\"/radar/languages-and-frameworks/vue-js\">Vue</a>, we've also seen the rise of <strong>component visual regression testing</strong>. This technique strikes a good balance between value and cost to ensure that no undesired visuals have been added to the application. In our experience, component visual regression testing presents fewer false positives and promotes a good architectural style. By using it with tools such as <a href=\"https://github.com/vitejs/vite\">Vite</a> and the webpack feature <a href=\"https://webpack.js.org/guides/hot-module-replacement/\">Hot Module Replacement (HMR)</a>, it could be seen as a paradigm shift for applying test-driven development to front-end development.</p>"
  },
  {
    "name": "Dragonfly",
    "ring": "Assess",
    "quadrant": "Platforms",
    "isNew": "TRUE",
    "description": "<p><strong><a href=\"https://github.com/dragonflydb/dragonfly\">Dragonfly</a></strong> is a new in-memory data store with compatible <a href=\"/radar/platforms/redis\">Redis</a> and Memcached APIs. It leverages the new Linux-specific <a href=\"https://github.com/axboe/liburing\">io_uring</a> API for I/O and implements <a href=\"https://dragonflydb.io/blog/2022/06/23/cache_design/\">novel algorithms and data structures</a> on top of a multithreaded, shared-nothing architecture. Because of these clever choices in implementation, Dragonfly achieves impressive results in performance. Although Redis continues to be our default choice for in-memory data store solutions, we do think Dragonfly is an interesting choice to assess.</p>"
  },
  {
    "name": "OrioleDB",
    "ring": "Hold",
    "quadrant": "Platforms",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://github.com/orioledb/orioledb/\">OrioleDB</a></strong> is a new storage engine for PostgreSQL. Our teams use PostgreSQL a lot, but its storage engine was originally designed for hard drives. Although there are several options to tune for modern hardware, it can be difficult and cumbersome to achieve optimal results. OrioleDB addresses these challenges by implementing a cloud-native storage engine with explicit support for solid-state drives (SSDs) and nonvolatile random-access memory (NVRAM). To try the new engine, first install the enhancement patches to the current <a href=\"https://www.postgresql.org/docs/current/tableam.html\">table access methods</a> and then install OrioleDB as a PostgreSQL extension. We believe OrioleDB has great potential to address several <a href=\"https://www.slideshare.net/AlexanderKorotkov/solving-postgresql-wicked-problems\">long-pending issues in PostgreSQL</a>, and we encourage you to carefully assess it.</p>"
  },
  {
    "name": "AWS Backup Vault Lock",
    "ring": "Trial",
    "quadrant": "Tools",
    "isNew": "TRUE",
    "description": "<p>When implementing robust, secure and reliable disaster recovery, it’s necessary to ensure that backups can't be deleted or altered before their expiry, either maliciously or accidentally. Previously, with AWS Backup, these policies and guarantees had to be implemented by hand. Recently, AWS has added the Vault Lock feature to ensure backups are immutable and untamperable. <a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\"><strong>AWS Backup Vault Lock</strong></a> enforces retention and deletion policies and prevents even those with administrator privileges from altering or deleting backup files. This has proved to be a valuable addition and fills a previously empty space.</p>"
  },
  {
    "name": "Databricks Overwatch",
    "ring": "Assess",
    "quadrant": "Tools",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://databrickslabs.github.io/overwatch/\">Databricks Overwatch</a></strong> is a Databricks Labs project that enables teams to analyze various operational metrics of Databricks workloads around cost, governance and performance with support to run what-if experiments. It's essentially a set of data pipelines that populate tables in Databricks, which can then be analyzed using tools like notebooks. Overwatch is very much a power tool; however, it's still in its early stages and it may take some effort to set it up — our use of it required Databricks solution architects to help set it up and populate a price reference table for cost calculations — but we expect adoption to get easier over time. The level of analysis made possible by Overwatch is deeper than what is allowed by cloud providers' cost analysis tools. For example, we were able to analyze the cost of job failures — recognizing that failing fast saves money compared to jobs that only fail near the final step — and break down the cost by various groupings (workspace, cluster, job, notebook, team). We also appreciated the improved operational visibility, as we could easily audit access controls around cluster configurations and analyze operational metrics like finding the longest running notebook or largest read/write volume. Overwatch can analyze historical data, but its real-time mode allows for alerting which helps you to add appropriate controls to your Databricks workloads.</p>"
  },
  {
    "name": "io-ts",
    "ring": "Adopt",
    "quadrant": "Tools",
    "isNew": "TRUE",
    "description": "<p>Our teams developing in <a href=\"/radar/languages-and-frameworks/typescript\">TypeScript</a> are finding <strong><a href=\"https://gcanti.github.io/io-ts/\">io-ts</a></strong> invaluable, especially when interacting with APIs that ultimately result in the creation of objects with specific types. When working with TypeScript, getting data into the bounds of the type system (i.e., from the aforementioned APIs) can lead to run-time errors that can be hard to find and debug. io-ts bridges the gap between compile-time type checking and run-time consumption of external data by providing encode and decode functions. Given the experiences of our teams and the elegance of its approach, we think io-ts is worth adopting.</p>"
  },
  {
    "name": "Carbon",
    "ring": "Hold",
    "quadrant": "Tools",
    "isNew": "FALSE",
    "description": "<p>We're seeing some interest in the <strong><a href=\"https://github.com/carbon-language/carbon-lang\">Carbon</a></strong> programming language. That doesn't come as a surprise: it has Google's backing and is presented as a natural successor to C++. In our opinion C++ can't be replaced fast enough as software engineers have shown, over the past decades, that writing safe and error-free C++ code is extremely difficult and time-consuming. While Carbon is an interesting concept with its focus on migration from C++, without a working compiler, it's clearly a long way from being usable and there are other modern programming languages that are good choices if you want to migrate from C++. It's too early to tell whether Carbon will become the natural successor to C++, but, from today's perspective, we recommend that teams look at <a href=\"/radar/languages-and-frameworks/rust\">Rust</a> and <a href=\"/radar/languages-and-frameworks/go-language\">Go</a> rather than postponing a migration because they're waiting for Carbon to arrive.</p>"
  },
  {
    "name": "CUPID",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p>How do you approach writing good code? How do you judge if you've written good code? As software developers, we're always looking for catchy rules, principles and patterns that we can use to share a language and values with each other when it comes to writing simple, easy-to-change code.</p><p>Daniel Terhorst-North has recently made a new attempt at creating such a checklist for good code. He argues that instead of sticking to a set of rules like <a href=\"https://en.wikipedia.org/wiki/SOLID\">SOLID</a>, using a set of properties to aim for is more generally applicable. He came up with what he calls the <strong><a href=\"https://dannorth.net/2022/02/10/cupid-for-joyful-coding/\">CUPID</a></strong> properties to describe what we should strive for to achieve \"joyful\" code: Code should be composable, follow the Unix philosophy and be predictable, idiomatic and domain based.</p>"
  },
  {
    "name": "GitHub push protection",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p>The accidental publication of secrets seems to be a perennial issue with tools such as <a href=\"/radar/tools/talisman\">Talisman</a> popping up to help with the problem. Before now, GitHub Enterprise Cloud users with an Advanced Security License could enable security scanning on their accounts, and any secrets (API keys, access tokens, credentials, etc.) that were accidentally committed and pushed would trigger an alert. <strong><a href=\"https://docs.github.com/en/enterprise-cloud@latest/code-security/secret-scanning/protecting-pushes-with-secret-scanning\">GitHub push protection</a></strong> takes this one step further, and brings it one step earlier in the development workflow, by blocking changes from being pushed at all if secrets are detected. This needs to be configured for the organization and applies, of course, only to license holders, but additional protection from publishing secrets is to be welcomed.</p>"
  },
  {
    "name": "Karpenter",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p>One of the fundamental capabilities of <a href=\"/radar/platforms/kubernetes\">Kubernetes</a> is its ability to automatically launch new pods when additional capacity is needed and shut them down when loads decrease. This horizontal autoscaling is a useful feature, but it can only work if the nodes needed to host the pods already exist. While <a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\">Cluster Autoscaler</a> can do some rudimentary cluster expansion triggered by pod failures, it has limited flexibility; <strong><a href=\"https://karpenter.sh/\">Karpenter</a></strong>, however, is an open-source <a href=\"/radar/tools/kubernetes-operators\">Kubernetes Operator</a> autoscaler with more smarts built in: it analyzes the current workloads and the pod scheduling constraints to automatically select an appropriate instance type and then start or stop it as needed. Karpenter is an operator in the spirit of tools like <a href=\"/radar/tools/crossplane\">Crossplane</a> that can provision cloud resources outside the cluster. Karpenter is an attractive companion to the autoscaling services cloud vendors provide natively with their managed Kubernetes clusters. For example, AWS now supports Karpenter as a first-class alternative in their EKS Cluster Autoscaler service.</p>"
  },
  {
    "name": "Mizu",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p><strong><a href=\"https://github.com/up9inc/mizu/tree/main\">Mizu</a></strong> is an API traffic viewer for <a href=\"/radar/platforms/kubernetes\">Kubernetes</a>. Unlike other tools, Mizu does not require instrumentation or code changes. It runs as a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\">DaemonSet</a> to inject a container at the node level in your Kubernetes cluster and performs tcpdump-like operations. We find it useful as a debugging tool, as it can observe all API communications across multiple protocols (REST, gRPC, <a href=\"/radar/platforms/apache-kafka\">Kafka</a>, AMQP and <a href=\"/radar/platforms/redis\">Redis</a>) in real time.</p>"
  },
  {
    "name": "Soda Core",
    "ring": "Assess",
    "quadrant": "Application",
    "isNew": "TRUE",
    "description": "<p><a href=\"https://www.soda.io/core\"><strong>Soda Core</strong></a> is an open-source data quality and observability tool. We talked about <a href=\"/radar/tools/great-expectations\">Great Expectations</a> previously in the Radar, and Soda Core is an alternative with a key difference — you express the data validations in a DSL called <a href=\"https://docs.soda.io/soda-cl/soda-cl-overview.html\">SodaCL</a> (previously called <a href=\"https://docs.soda.io/soda-sql/overview.html\">Soda SQL</a>) as opposed to Python functions. Once the validations are written, it can be executed as part of a <a href=\"https://docs.soda.io/soda-core/orchestrate-scans.html\">data pipeline</a> or <a href=\"https://docs.soda.io/soda-core/programmatic.html\">scheduled to run programmatically</a>. As we become increasingly data-driven, it's critical to maintain data quality, and we encourage you to assess Soda Core.</p>"
  },
  {
    "name": "Teller",
    "ring": "Assess",
    "quadrant": "Application",
    "isNew": "TRUE",
    "description": "<p><strong><a href=\"https://github.com/tellerops/teller\">Teller</a></strong> is an open-source universal secret manager for developers that ensures the correct environment variables are set when starting an application. However, it's not a vault itself — it's a CLI tool that connects to a variety of sources, ranging from cloud secrets providers to third-party solutions like <a href=\"/radar/tools/hashicorp-vault\">HashiCorp Vault</a> to local environment files. Teller has additional functionality to scan for vault-kept secrets in your code, to redact secrets from logs, to detect drift between secrets providers and to sync between them. Given the sensitivity of accessing secrets, we can't emphasize enough the need to secure the supply chain for open-source dependencies, but we appreciate how easy the CLI is to use in local development environments, CI/CD pipelines and deployment automation.</p>"
  }
]
