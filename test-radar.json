[
  {
    "name": "Path-to-production mapping",
    "ring": "Adopt",
    "quadrant": "Platform",
    "isNew": "TRUE",
    "description": "<p>Although <strong>path-to-production mapping</strong> has been a near-universal practice at Thoughtworks since codifying <em><a href=\"https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912\">Continuous Delivery</a></em>, we often come across organizations unfamiliar with the practice. The activity is most often done in a workshop with a cross-functional group of people —  that includes everyone involved in designing, developing, releasing and operating the software — around a shared whiteboard (or virtual equivalent). First, the steps in the process are listed in order, from the developer workstation all the way to production. Then, a facilitated session is used to capture further information and pain points. The most common technique we see is based on <a href=\"https://en.wikipedia.org/wiki/Value-stream_mapping\">value-stream mapping</a>, although plenty of <a href=\"https://caroli.org/en/path-to-production/\">process map</a> variants are equally valuable. The activity is often eye-opening for many of the participants, as they identify delays, risks and inconsistencies and continue to use the visual representation for the continuous improvement of the build and deploy process. We consider this technique so foundational that we were surprised to discover we hadn't blipped it before.</p>"
  },
  {
    "name": "Component visual regression testing",
    "ring": "Trial",
    "quadrant": "Platform",
    "isNew": "FALSE",
    "description": "<p><a href=\"/radar/tools/visual-regression-testing-tools\">Visual regression testing</a> is a useful and powerful tool to have in your toolbox, but it has a significant cost given it's done for the entire page. With the rise of component-based frameworks such as <a href=\"/radar/languages-and-frameworks/react-js\">React</a> and <a href=\"/radar/languages-and-frameworks/vue-js\">Vue</a>, we've also seen the rise of <strong>component visual regression testing</strong>. This technique strikes a good balance between value and cost to ensure that no undesired visuals have been added to the application. In our experience, component visual regression testing presents fewer false positives and promotes a good architectural style. By using it with tools such as <a href=\"https://github.com/vitejs/vite\">Vite</a> and the webpack feature <a href=\"https://webpack.js.org/guides/hot-module-replacement/\">Hot Module Replacement (HMR)</a>, it could be seen as a paradigm shift for applying test-driven development to front-end development.</p>"
  },
  {
    "name": "Dragonfly",
    "ring": "Assess",
    "quadrant": "Platforms",
    "isNew": "TRUE",
    "description": "<p><strong><a href=\"https://github.com/dragonflydb/dragonfly\">Dragonfly</a></strong> is a new in-memory data store with compatible <a href=\"/radar/platforms/redis\">Redis</a> and Memcached APIs. It leverages the new Linux-specific <a href=\"https://github.com/axboe/liburing\">io_uring</a> API for I/O and implements <a href=\"https://dragonflydb.io/blog/2022/06/23/cache_design/\">novel algorithms and data structures</a> on top of a multithreaded, shared-nothing architecture. Because of these clever choices in implementation, Dragonfly achieves impressive results in performance. Although Redis continues to be our default choice for in-memory data store solutions, we do think Dragonfly is an interesting choice to assess.</p>"
  },
  {
    "name": "OrioleDB",
    "ring": "Hold",
    "quadrant": "Platforms",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://github.com/orioledb/orioledb/\">OrioleDB</a></strong> is a new storage engine for PostgreSQL. Our teams use PostgreSQL a lot, but its storage engine was originally designed for hard drives. Although there are several options to tune for modern hardware, it can be difficult and cumbersome to achieve optimal results. OrioleDB addresses these challenges by implementing a cloud-native storage engine with explicit support for solid-state drives (SSDs) and nonvolatile random-access memory (NVRAM). To try the new engine, first install the enhancement patches to the current <a href=\"https://www.postgresql.org/docs/current/tableam.html\">table access methods</a> and then install OrioleDB as a PostgreSQL extension. We believe OrioleDB has great potential to address several <a href=\"https://www.slideshare.net/AlexanderKorotkov/solving-postgresql-wicked-problems\">long-pending issues in PostgreSQL</a>, and we encourage you to carefully assess it.</p>"
  },
  {
    "name": "AWS Backup Vault Lock",
    "ring": "Trial",
    "quadrant": "Tools",
    "isNew": "TRUE",
    "description": "<p>When implementing robust, secure and reliable disaster recovery, it’s necessary to ensure that backups can't be deleted or altered before their expiry, either maliciously or accidentally. Previously, with AWS Backup, these policies and guarantees had to be implemented by hand. Recently, AWS has added the Vault Lock feature to ensure backups are immutable and untamperable. <a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\"><strong>AWS Backup Vault Lock</strong></a> enforces retention and deletion policies and prevents even those with administrator privileges from altering or deleting backup files. This has proved to be a valuable addition and fills a previously empty space.</p>"
  },
  {
    "name": "Databricks Overwatch",
    "ring": "Assess",
    "quadrant": "Tools",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://databrickslabs.github.io/overwatch/\">Databricks Overwatch</a></strong> is a Databricks Labs project that enables teams to analyze various operational metrics of Databricks workloads around cost, governance and performance with support to run what-if experiments. It's essentially a set of data pipelines that populate tables in Databricks, which can then be analyzed using tools like notebooks. Overwatch is very much a power tool; however, it's still in its early stages and it may take some effort to set it up — our use of it required Databricks solution architects to help set it up and populate a price reference table for cost calculations — but we expect adoption to get easier over time. The level of analysis made possible by Overwatch is deeper than what is allowed by cloud providers' cost analysis tools. For example, we were able to analyze the cost of job failures — recognizing that failing fast saves money compared to jobs that only fail near the final step — and break down the cost by various groupings (workspace, cluster, job, notebook, team). We also appreciated the improved operational visibility, as we could easily audit access controls around cluster configurations and analyze operational metrics like finding the longest running notebook or largest read/write volume. Overwatch can analyze historical data, but its real-time mode allows for alerting which helps you to add appropriate controls to your Databricks workloads.</p>"
  },
  {
    "name": "io-ts",
    "ring": "Adopt",
    "quadrant": "Tools",
    "isNew": "TRUE",
    "description": "<p>Our teams developing in <a href=\"/radar/languages-and-frameworks/typescript\">TypeScript</a> are finding <strong><a href=\"https://gcanti.github.io/io-ts/\">io-ts</a></strong> invaluable, especially when interacting with APIs that ultimately result in the creation of objects with specific types. When working with TypeScript, getting data into the bounds of the type system (i.e., from the aforementioned APIs) can lead to run-time errors that can be hard to find and debug. io-ts bridges the gap between compile-time type checking and run-time consumption of external data by providing encode and decode functions. Given the experiences of our teams and the elegance of its approach, we think io-ts is worth adopting.</p>"
  },
  {
    "name": "Carbon",
    "ring": "Hold",
    "quadrant": "Tools",
    "isNew": "FALSE",
    "description": "<p>We're seeing some interest in the <strong><a href=\"https://github.com/carbon-language/carbon-lang\">Carbon</a></strong> programming language. That doesn't come as a surprise: it has Google's backing and is presented as a natural successor to C++. In our opinion C++ can't be replaced fast enough as software engineers have shown, over the past decades, that writing safe and error-free C++ code is extremely difficult and time-consuming. While Carbon is an interesting concept with its focus on migration from C++, without a working compiler, it's clearly a long way from being usable and there are other modern programming languages that are good choices if you want to migrate from C++. It's too early to tell whether Carbon will become the natural successor to C++, but, from today's perspective, we recommend that teams look at <a href=\"/radar/languages-and-frameworks/rust\">Rust</a> and <a href=\"/radar/languages-and-frameworks/go-language\">Go</a> rather than postponing a migration because they're waiting for Carbon to arrive.</p>"
  },
  {
    "name": "Team cognitive load",
    "ring": "Adopt",
    "quadrant": "Application",
    "isNew": "TRUE",
    "description": "<p>Team interaction is a key concept when redesigning an organization for business agility and speed. These interactions will be reflected in the software being built (see <a href=\"https://www.thoughtworks.com/about-us/news/2021/latest-thoughtworks-technology-radar-proclaims---embrace-conway-\">Conway's Law</a>) and indicate how effectively teams can autonomously deliver value to their customers. Our advice is to be intentional about how teams are designed and how they interact. Because we believe that organizational design and team interactions evolve over time, we think it's particularly important to measure and keep track of the <strong>team cognitive load</strong>, which indicates how easy or difficult teams find building, testing and maintaining their services. We've been using a <a href=\"https://github.com/TeamTopologies/Team-Cognitive-Load-Assessment\">template</a> to assess team cognitive load that is based on ideas by the authors of the <em><a href=\"https://teamtopologies.com/book\">Team Topologies</a></em> book.</p>"
  },
  {
    "name": "Threat modeling",
    "ring": "Adopt",
    "quadrant": "Application",
    "isNew": "FALSE",
    "description": "<p>We continue to recommend that teams carry out <strong><a href=\"https://www.owasp.org/index.php/Category:Threat_Modeling\">threat modeling</a></strong> — a set of Platform to help you identify and classify potential threats during the development process — but we want to emphasize that this is not a one-off activity only done at the start of projects; teams need to avoid the <a href=\"/radar/Platform/security-sandwich\">security sandwich</a>. This is because throughout the lifetime of any software, new threats will emerge and existing ones will continue to evolve thanks to external events and ongoing changes to requirements and architecture. This means that threat modeling needs to be repeated periodically — the frequency of repetition will depend on the circumstances and will need to consider factors such as the cost of running the exercise and the potential risk to the business. When used in conjunction with other Platform, such as establishing cross-functional security requirements to address common risks in the project's technologies and using automated security scanners, threat modeling can be a powerful asset.</p>"
  },
  {
    "name": "Backstage",
    "ring": "Adopt",
    "quadrant": "Application",
    "isNew": "FALSE",
    "description": "<p>In an increasingly digital world, improving developer effectiveness in large organizations is often a core concern of senior leaders. We've seen enough value with developer portals in general and <strong><a href=\"https://backstage.io/\">Backstage</a></strong> in particular that we're happy to recommend it in Adopt. Backstage is an open-source developer portal platform created by Spotify that improves discovery of software assets across the organization. It uses Markdown <a href=\"https://backstage.io/docs/features/techdocs/techdocs-overview\">TechDocs</a> that live alongside the code for each service, which nicely balances the needs of centralized discovery with the need for distributed ownership of assets. Backstage supports software templates to accelerate new development and a plugin architecture that allows for extensibility and adaptability into an organization's infrastructure ecosystem. <a href=\"https://backstage.io/docs/features/software-catalog/software-catalog-overview\">Backstage Service Catalog</a> uses YAML files to track ownership and metadata for all the software in an organization's ecosystem; it even lets you track third-party SaaS software, which usually requires tracking ownership.</p>"
  },
  {
    "name": "Delta Lake",
    "ring": "Adopt",
    "quadrant": "Application",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://delta.io/\">Delta Lake</a></strong> is an <a href=\"https://github.com/delta-io/delta\">open-source storage layer</a>, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled <a href=\"/radar/Platform/data-lake\">data lake</a> or <a href=\"/radar/Platform/data-mesh\">data mesh</a> projects, our teams prefer using Delta Lake storage over the direct use of file storage types such as <a href=\"https://aws.amazon.com/s3/\">AWS S3</a> or <a href=\"https://azure.microsoft.com/en-au/services/storage/data-lake-storage/\">ADLS</a>. Until recently, Delta Lake has been a closed proprietary product from Databricks, but it's now open source and accessible to non-Databricks platforms. However, our recommendation of Delta Lake as a default choice currently extends only to Databricks projects that use <a href=\"https://parquet.apache.org/\">Parquet</a> file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark <a href=\"https://docs.databricks.com/delta/delta-batch.html\">batch</a> and <a href=\"https://docs.databricks.com/delta/delta-streaming.html\">micro-batch</a> APIs very helpful, particularly features such as <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\">time travel</a> (accessing data at a particular point in time or commit reversion) as well as <a href=\"https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\">schema evolution</a> support on write.</p>"
  },
  {
    "name": "git-together",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p>We're always looking for ways to remove small frictions from pair programming, which is why we're excited by <a href=\"https://github.com/kejadlen/git-together\"><strong>git-together</strong></a>, a tool written in Rust that simplifies git commit attribution during pairing. By aliasing <code>git-together</code> as <code>git</code>, the tool allows you to add extensions to <code>git config</code> that capture committer information, aliasing each committer by their initials. Changing pairs (or switching to soloing or mob programming) requires you to run <code>git with</code>, followed by the initials of the pair (for example: <code>git with bb cc</code>), allowing you to resume your regular git workflow afterward. Every time you commit, git-together will rotate through the pair as the official author that git stores, and it will automatically add any other authors to the bottom of the commit message. The configuration can be checked in with the repo, allowing git-together to work automatically after cloning a repo.</p>"
  },
  {
    "name": "Harness Cloud Cost Management",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p><strong><a href=\"https://harness.io/products/cloud-cost\">Harness Cloud Cost Management</a></strong> is a commercial tool that works across all three of the major cloud providers and their managed <a href=\"/radar/platforms/kubernetes\">Kubernetes</a> clusters to help visualize and manage cloud costs. The product calculates a cost efficiency score by looking at idle resources as well as resources not allocated to any workload and uses historical trends to help optimize resource allocation. The dashboards highlight cost spikes and allow a user to register unexpected anomalies, which are then fed into their reinforcement learning algorithm around anomaly detection. Cloud Cost Management can recommend adjustments to limits for memory and CPU usage, with options to optimize for either cost or performance. \"Perspectives\" allows you to group costs based on organizationally defined filters (which could correspond to business units, teams or products) and automate report distribution to bring visibility into cloud spend. We believe Cloud Cost Management offers a compelling feature set to help organizations mature their FinOps practices.</p>"
  },
  {
    "name": "Infracost",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p>We continue to see organizations move to the cloud without properly understanding how they will track ongoing spend. We previously blipped <a href=\"/radar/Platform/run-cost-as-architecture-fitness-function\">run cost as architecture fitness function</a>, and <a href=\"https://infracost.io/\"><strong>Infracost</strong></a> is a tool that aims to make these cloud cost trade-offs visible in Terraform pull requests. It's open-source software and available for macOS, Linux, Windows and Docker and supports pricing for AWS, GCP and Microsoft Azure out of the box. It also provides a public API that can be queried for current cost data. We remain excited by its potential, especially when it comes to gaining better cost visibility in the IDE.</p>"
  },
  {
    "name": "Karpenter",
    "ring": "Assess",
    "quadrant": "Frameworks",
    "isNew": "TRUE",
    "description": "<p>One of the fundamental capabilities of <a href=\"/radar/platforms/kubernetes\">Kubernetes</a> is its ability to automatically launch new pods when additional capacity is needed and shut them down when loads decrease. This horizontal autoscaling is a useful feature, but it can only work if the nodes needed to host the pods already exist. While <a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\">Cluster Autoscaler</a> can do some rudimentary cluster expansion triggered by pod failures, it has limited flexibility; <strong><a href=\"https://karpenter.sh/\">Karpenter</a></strong>, however, is an open-source <a href=\"/radar/tools/kubernetes-operators\">Kubernetes Operator</a> autoscaler with more smarts built in: it analyzes the current workloads and the pod scheduling constraints to automatically select an appropriate instance type and then start or stop it as needed. Karpenter is an operator in the spirit of tools like <a href=\"/radar/tools/crossplane\">Crossplane</a> that can provision cloud resources outside the cluster. Karpenter is an attractive companion to the autoscaling services cloud vendors provide natively with their managed Kubernetes clusters. For example, AWS now supports Karpenter as a first-class alternative in their EKS Cluster Autoscaler service.</p>"
  }
]
